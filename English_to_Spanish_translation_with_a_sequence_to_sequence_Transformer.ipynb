{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English-to-Spanish translation with a sequence-to-sequence Transformer",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedSSoliman/English-to-Spanish-translation-with-a-sequence-to-sequence-Transformer/blob/main/English_to_Spanish_translation_with_a_sequence_to_sequence_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Kw0VrgR0YAY"
      },
      "source": [
        "# English-to-Spanish translation with a sequence-to-sequence Transformer\n",
        "\n",
        "\n",
        "\n",
        "**Description:** Implementing a sequence-to-sequene Transformer and training it on a machine translation task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I1voMif0YAe"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this example, we'll build a sequence-to-sequence Transformer model, which\n",
        "we'll train on an English-to-Spanish machine translation task.\n",
        "\n",
        "You'll learn how to:\n",
        "\n",
        "- Vectorize text using the Keras `TextVectorization` layer.\n",
        "- Implement a `TransformerEncoder` layer, a `TransformerDecoder` layer,\n",
        "and a `PositionalEmbedding` layer.\n",
        "- Prepare data for training a sequence-to-sequence model.\n",
        "- Use the trained model to generate translations of never-seen-before\n",
        "input sentences (sequence-to-sequence inference).\n",
        "\n",
        "The code featured here is adapted from the book\n",
        "[Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition)\n",
        "(chapter 11: Deep learning for text).\n",
        "The present example is fairly barebones, so for detailed explanations of\n",
        "how each building block works, as well as the theory behind Transformers,\n",
        "I recommend reading the book."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7Tr0Qzg0YAf"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDJphKrq0YAg"
      },
      "source": [
        "import pathlib\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPr7jnT00YAj"
      },
      "source": [
        "## Downloading the data\n",
        "\n",
        "We'll be working with an English-to-Spanish translation dataset\n",
        "provided by [Anki](https://www.manythings.org/anki/). Let's download it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bcNIu1E0YAk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "510e1fcb-a569-4910-e6b8-41f6011ed8fc"
      },
      "source": [
        "text_file = keras.utils.get_file(\n",
        "    fname=\"spa-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n",
            "2654208/2638744 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxHpCBYN0YAl"
      },
      "source": [
        "## Parsing the data\n",
        "\n",
        "Each line contains an English sentence and its corresponding Spanish sentence.\n",
        "The English sentence is the *source sequence* and Spanish one is the *target sequence*.\n",
        "We prepend the token `\"[start]\"` and we append the token `\"[end]\"` to the Spanish sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2--7ptN0YAm"
      },
      "source": [
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, spa = line.split(\"\\t\")\n",
        "    spa = \"[start] \" + spa + \" [end]\"\n",
        "    text_pairs.append((eng, spa))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ineVoHzM0YAm"
      },
      "source": [
        "Here's what our sentence pairs look like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5syyEzgy0YAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4a22e3f-fa40-4d16-8b36-e9baacb2c57c"
      },
      "source": [
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Do you still really want to go?', '[start] ¿Estás seguro de que todavía quieres ir? [end]')\n",
            "('Can you drive a car?', '[start] ¿Puedes conducir un auto? [end]')\n",
            "(\"I don't have much time now.\", '[start] Ahora no tengo mucho tiempo. [end]')\n",
            "('This large sofa would be out of place in a small room.', '[start] Este gran sofá podría ser inadecuado para una habitación pequeña. [end]')\n",
            "('Wait one second.', '[start] Esperen un segundo. [end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOyHYCbw0YAp"
      },
      "source": [
        "Now, let's split the sentence pairs into a training set, a validation set,\n",
        "and a test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcDBqGkI0YAp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b76e9427-2428-4f34-896f-737ef2942f82"
      },
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "118964 total pairs\n",
            "83276 training pairs\n",
            "17844 validation pairs\n",
            "17844 test pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK-90iSg0YAq"
      },
      "source": [
        "## Vectorizing the text data\n",
        "\n",
        "We'll use two instances of the `TextVectorization` layer to vectorize the text\n",
        "data (one for English and one for Spanish),\n",
        "that is to say, to turn the original strings into integer sequences\n",
        "where each integer represents the index of a word in a vocabulary.\n",
        "\n",
        "The English layer will use the default string standardization (strip punctuation characters)\n",
        "and splitting scheme (split on whitespace), while\n",
        "the Spanish layer will use a custom standardization, where we add the character\n",
        "`\"¿\"` to the set of punctuation characters to be stripped.\n",
        "\n",
        "Note: in a production-grade machine translation model, I would not recommend\n",
        "stripping the punctuation characters in either language. Instead, I would recommend turning\n",
        "each punctuation character into its own token,\n",
        "which you could achieve by providing a custom `split` function to the `TextVectorization` layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vGXBhoF0YAr"
      },
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "\n",
        "eng_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",
        ")\n",
        "spa_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_spa_texts = [pair[1] for pair in train_pairs]\n",
        "eng_vectorization.adapt(train_eng_texts)\n",
        "spa_vectorization.adapt(train_spa_texts)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7stNAXc0YAs"
      },
      "source": [
        "Next, we'll format our datasets.\n",
        "\n",
        "At each training step, the model will seek to predict target words N+1 (and beyond)\n",
        "using the source sentence and the target words 0 to N.\n",
        "\n",
        "As such, the training dataset will yield a tuple `(inputs, targets)`, where:\n",
        "\n",
        "- `inputs` is a dictionary with the keys `encoder_inputs` and `decoder_inputs`.\n",
        "`encoder_inputs` is the vectorized source sentence and `encoder_inputs` is the target sentence \"so far\",\n",
        "that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target sentence.\n",
        "- `target` is the target sentence offset by one step:\n",
        "it provides the next words in the target sentence -- what the model will try to predict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ddf24X60YAs"
      },
      "source": [
        "\n",
        "def format_dataset(eng, spa):\n",
        "    eng = eng_vectorization(eng)\n",
        "    spa = spa_vectorization(spa)\n",
        "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": spa[:, :-1],}, spa[:, 1:])\n",
        "\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysOYgAUz0YAt"
      },
      "source": [
        "Let's take a quick look at the sequence shapes\n",
        "(we have batches of 64 pairs, and all sequences are 20 steps long):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af53gcq80YAt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb509631-a717-455f-ee88-af3ca4fa9a90"
      },
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (64, 20)\n",
            "inputs[\"decoder_inputs\"].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdPUhM1r0YAu"
      },
      "source": [
        "## Building the model\n",
        "\n",
        "Our sequence-to-sequence Transformer consists of a `TransformerEncoder`\n",
        "and a `TransformerDecoder` chained together. To make the model aware of word order,\n",
        "we also use a `PositionalEmbedding` layer.\n",
        "\n",
        "The source sequence will be pass to the `TransformerEncoder`,\n",
        "which will produce a new representation of it.\n",
        "This new representation will then be passed\n",
        "to the `TransformerDecoder`, together with the target sequence so far (target words 0 to N).\n",
        "The `TransformerDecoder` will then seek to predict the next words in the target sequence (N+1 and beyond).\n",
        "\n",
        "A key detail that makes this possible is causal masking\n",
        "(see method `get_causal_attention_mask()` on the `TransformerDecoder`).\n",
        "The `TransformerDecoder` sees the entire sequences at once, and thus we must make\n",
        "sure that it only uses information from target tokens 0 to N when predicting token N+1\n",
        "(otherwise, it could use information from the future, which would\n",
        "result in a model that cannot be used at inference time)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix2ljMyJ0YAu"
      },
      "source": [
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super(TransformerDecoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur0r-jEn0YAv"
      },
      "source": [
        "Next, we assemble the end-to-end model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43pphrcq0YAw"
      },
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1GQd8V_0YAw"
      },
      "source": [
        "## Training our model\n",
        "\n",
        "We'll use accuracy as a quick way to monitor training progress on the validation data.\n",
        "Note that machine translation typically uses BLEU scores as well as other metrics, rather than accuracy.\n",
        "\n",
        "Here we only train for 1 epoch, but to get the model to actually converge\n",
        "you should train for at least 30 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPdXuF5n0YAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "206e2fac-7383-49ef-fc77-807313d35552"
      },
      "source": [
        "epochs = 50  # This should be at least 30 for convergence\n",
        "\n",
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "positional_embedding (Positiona (None, None, 256)    3845120     encoder_inputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "transformer_encoder (Transforme (None, None, 256)    3155456     positional_embedding[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "model_1 (Functional)            (None, None, 15000)  12959640    decoder_inputs[0][0]             \n",
            "                                                                 transformer_encoder[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 19,960,216\n",
            "Trainable params: 19,960,216\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "1302/1302 [==============================] - 95s 67ms/step - loss: 1.6423 - accuracy: 0.4299 - val_loss: 1.2912 - val_accuracy: 0.5197\n",
            "Epoch 2/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 1.3211 - accuracy: 0.5396 - val_loss: 1.1534 - val_accuracy: 0.5753\n",
            "Epoch 3/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 1.1776 - accuracy: 0.5845 - val_loss: 1.0798 - val_accuracy: 0.6035\n",
            "Epoch 4/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 1.0930 - accuracy: 0.6153 - val_loss: 1.0418 - val_accuracy: 0.6218\n",
            "Epoch 5/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 1.0455 - accuracy: 0.6368 - val_loss: 1.0193 - val_accuracy: 0.6345\n",
            "Epoch 6/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 1.0157 - accuracy: 0.6522 - val_loss: 1.0035 - val_accuracy: 0.6417\n",
            "Epoch 7/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.9937 - accuracy: 0.6646 - val_loss: 0.9984 - val_accuracy: 0.6458\n",
            "Epoch 8/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.9753 - accuracy: 0.6752 - val_loss: 0.9943 - val_accuracy: 0.6507\n",
            "Epoch 9/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.9597 - accuracy: 0.6840 - val_loss: 0.9879 - val_accuracy: 0.6552\n",
            "Epoch 10/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.9456 - accuracy: 0.6909 - val_loss: 0.9939 - val_accuracy: 0.6553\n",
            "Epoch 11/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.9329 - accuracy: 0.6979 - val_loss: 0.9902 - val_accuracy: 0.6608\n",
            "Epoch 12/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.9200 - accuracy: 0.7041 - val_loss: 0.9952 - val_accuracy: 0.6594\n",
            "Epoch 13/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.9082 - accuracy: 0.7097 - val_loss: 0.9941 - val_accuracy: 0.6601\n",
            "Epoch 14/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.8964 - accuracy: 0.7146 - val_loss: 0.9927 - val_accuracy: 0.6639\n",
            "Epoch 15/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.8852 - accuracy: 0.7197 - val_loss: 1.0051 - val_accuracy: 0.6568\n",
            "Epoch 16/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.8740 - accuracy: 0.7241 - val_loss: 1.0044 - val_accuracy: 0.6633\n",
            "Epoch 17/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.8630 - accuracy: 0.7288 - val_loss: 1.0061 - val_accuracy: 0.6652\n",
            "Epoch 18/50\n",
            "1302/1302 [==============================] - 87s 66ms/step - loss: 0.8521 - accuracy: 0.7332 - val_loss: 1.0091 - val_accuracy: 0.6635\n",
            "Epoch 19/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.8416 - accuracy: 0.7371 - val_loss: 1.0131 - val_accuracy: 0.6631\n",
            "Epoch 20/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.8306 - accuracy: 0.7417 - val_loss: 1.0203 - val_accuracy: 0.6649\n",
            "Epoch 21/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.8216 - accuracy: 0.7449 - val_loss: 1.0251 - val_accuracy: 0.6626\n",
            "Epoch 22/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.8130 - accuracy: 0.7479 - val_loss: 1.0240 - val_accuracy: 0.6642\n",
            "Epoch 23/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.8027 - accuracy: 0.7521 - val_loss: 1.0434 - val_accuracy: 0.6645\n",
            "Epoch 24/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.7948 - accuracy: 0.7552 - val_loss: 1.0362 - val_accuracy: 0.6676\n",
            "Epoch 25/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.7865 - accuracy: 0.7581 - val_loss: 1.0377 - val_accuracy: 0.6663\n",
            "Epoch 26/50\n",
            "1302/1302 [==============================] - 87s 66ms/step - loss: 0.7775 - accuracy: 0.7614 - val_loss: 1.0424 - val_accuracy: 0.6649\n",
            "Epoch 27/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.7712 - accuracy: 0.7633 - val_loss: 1.0546 - val_accuracy: 0.6653\n",
            "Epoch 28/50\n",
            "1302/1302 [==============================] - 87s 66ms/step - loss: 0.7633 - accuracy: 0.7667 - val_loss: 1.0576 - val_accuracy: 0.6647\n",
            "Epoch 29/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.7554 - accuracy: 0.7696 - val_loss: 1.0641 - val_accuracy: 0.6639\n",
            "Epoch 30/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.7488 - accuracy: 0.7716 - val_loss: 1.0561 - val_accuracy: 0.6656\n",
            "Epoch 31/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.7419 - accuracy: 0.7738 - val_loss: 1.0688 - val_accuracy: 0.6659\n",
            "Epoch 32/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.7357 - accuracy: 0.7763 - val_loss: 1.0648 - val_accuracy: 0.6631\n",
            "Epoch 33/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.7303 - accuracy: 0.7785 - val_loss: 1.0739 - val_accuracy: 0.6653\n",
            "Epoch 34/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.7234 - accuracy: 0.7804 - val_loss: 1.0724 - val_accuracy: 0.6648\n",
            "Epoch 35/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.7178 - accuracy: 0.7822 - val_loss: 1.0829 - val_accuracy: 0.6650\n",
            "Epoch 36/50\n",
            "1302/1302 [==============================] - 86s 66ms/step - loss: 0.7135 - accuracy: 0.7843 - val_loss: 1.0890 - val_accuracy: 0.6642\n",
            "Epoch 37/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.7078 - accuracy: 0.7860 - val_loss: 1.0995 - val_accuracy: 0.6652\n",
            "Epoch 38/50\n",
            "1302/1302 [==============================] - 87s 66ms/step - loss: 0.7024 - accuracy: 0.7880 - val_loss: 1.1170 - val_accuracy: 0.6598\n",
            "Epoch 39/50\n",
            "1302/1302 [==============================] - 87s 66ms/step - loss: 0.6976 - accuracy: 0.7894 - val_loss: 1.1086 - val_accuracy: 0.6630\n",
            "Epoch 40/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.6917 - accuracy: 0.7922 - val_loss: 1.1237 - val_accuracy: 0.6634\n",
            "Epoch 41/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.6877 - accuracy: 0.7933 - val_loss: 1.1253 - val_accuracy: 0.6652\n",
            "Epoch 42/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.6839 - accuracy: 0.7944 - val_loss: 1.1157 - val_accuracy: 0.6634\n",
            "Epoch 43/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.6796 - accuracy: 0.7957 - val_loss: 1.1212 - val_accuracy: 0.6624\n",
            "Epoch 44/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.6753 - accuracy: 0.7973 - val_loss: 1.1285 - val_accuracy: 0.6580\n",
            "Epoch 45/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.6708 - accuracy: 0.7988 - val_loss: 1.1358 - val_accuracy: 0.6634\n",
            "Epoch 46/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.6677 - accuracy: 0.7999 - val_loss: 1.1276 - val_accuracy: 0.6657\n",
            "Epoch 47/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.6639 - accuracy: 0.8012 - val_loss: 1.1329 - val_accuracy: 0.6633\n",
            "Epoch 48/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.6605 - accuracy: 0.8023 - val_loss: 1.1504 - val_accuracy: 0.6610\n",
            "Epoch 49/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.6574 - accuracy: 0.8032 - val_loss: 1.1284 - val_accuracy: 0.6643\n",
            "Epoch 50/50\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 0.6530 - accuracy: 0.8047 - val_loss: 1.1540 - val_accuracy: 0.6665\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efe8adb1690>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqyNa9bL0YAy"
      },
      "source": [
        "## Decoding test sentences\n",
        "\n",
        "Finally, let's demonstrate how to translate brand new English sentences.\n",
        "We simply feed into the model the vectorized English sentence\n",
        "as well as the target token `\"[start]\"`, then we repeatedly generated the next token, until\n",
        "we hit the token `\"[end]\"`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0ZF1-WG0YAy"
      },
      "source": [
        "spa_vocab = spa_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNXJWX4N6uFT",
        "outputId": "e6b337ab-0d31-4cd7-894e-615f750515d2"
      },
      "source": [
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(30):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequence(input_sentence)\n",
        "    print(\"input_sentence: \\n\", input_sentence, \"\\n\")\n",
        "    print(\"translated_sentence: \\n\", translated)\n",
        "    print(\"-\"*100)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_sentence: \n",
            " He often gets worked up over little things. \n",
            "\n",
            "translated_sentence: \n",
            " [start] a menudo la [UNK] al respecto [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " Do you have some milk? \n",
            "\n",
            "translated_sentence: \n",
            " [start] te queda algo de leche [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " He is a doctor. \n",
            "\n",
            "translated_sentence: \n",
            " [start] Él es médico [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " Your poor memory is due to poor listening habits. \n",
            "\n",
            "translated_sentence: \n",
            " [start] tu mala memoria pasa porque no sabes escuchar [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " She was a pioneer in this field. \n",
            "\n",
            "translated_sentence: \n",
            " [start] ella era una [UNK] en este campo [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " I convinced Tom that he should go to Boston with Mary. \n",
            "\n",
            "translated_sentence: \n",
            " [start] tenía miedo de que tom debería ir a boston con mary [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " It is wrong to think that men are superior to women. \n",
            "\n",
            "translated_sentence: \n",
            " [start] está mal de ese tipo que las hombres es [UNK] a las mujeres [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " Who was where? \n",
            "\n",
            "translated_sentence: \n",
            " [start] quién estaba donde [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " I can't live that kind of life. \n",
            "\n",
            "translated_sentence: \n",
            " [start] no puedo vivir esa clase de vida [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " They began to climb the hill. \n",
            "\n",
            "translated_sentence: \n",
            " [start] ellos se [UNK] a esta colina [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " You like English, don't you? \n",
            "\n",
            "translated_sentence: \n",
            " [start] te gusta el inglés no [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " The money is in the well. \n",
            "\n",
            "translated_sentence: \n",
            " [start] el dinero está en el bien [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " I'm halfway through this crime novel. \n",
            "\n",
            "translated_sentence: \n",
            " [start] estoy terminado de 10 en aquel pájaro [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " This house is not for sale. \n",
            "\n",
            "translated_sentence: \n",
            " [start] esta casa no está en la bicicleta [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " To our surprise, her prediction came true. \n",
            "\n",
            "translated_sentence: \n",
            " [start] para nuestra sorpresa su sorpresa se [UNK] mucho [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " I get up at six every day. \n",
            "\n",
            "translated_sentence: \n",
            " [start] cada día [UNK] a las seis [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " I asked her to make four copies of the letter. \n",
            "\n",
            "translated_sentence: \n",
            " [start] le pedí que [UNK] a su carta [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " I am badly in need of your help. \n",
            "\n",
            "translated_sentence: \n",
            " [start] [UNK] es todo lo que [UNK] tu ayuda [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " The silver balls are around the red ball. \n",
            "\n",
            "translated_sentence: \n",
            " [start] las [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " Every cloud has a silver lining. \n",
            "\n",
            "translated_sentence: \n",
            " [start] toda una posibilidad de comida me [UNK] [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " We’ve been waiting for hours. \n",
            "\n",
            "translated_sentence: \n",
            " [start] ha estado esperando todo el [UNK] [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " I didn't say a word. \n",
            "\n",
            "translated_sentence: \n",
            " [start] no dije una palabra [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " I was very worried about her. \n",
            "\n",
            "translated_sentence: \n",
            " [start] estaba muy preocupado por ella [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " Did you hear about the big earthquake in Japan? \n",
            "\n",
            "translated_sentence: \n",
            " [start] acerca de cuánto grandes [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " The villagers had a bias against any newcomer. \n",
            "\n",
            "translated_sentence: \n",
            " [start] los a los a los a los a de de a mí el el el a [UNK] [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " She earns more than she spends. \n",
            "\n",
            "translated_sentence: \n",
            " [start] ella al [UNK] más que lo pase [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " I completely forgot. \n",
            "\n",
            "translated_sentence: \n",
            " [start] me ha olvidado [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " I don't like to eat garlic in the morning. \n",
            "\n",
            "translated_sentence: \n",
            " [start] no me gusta comer al béisbol en la mañana [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " Just take one. \n",
            "\n",
            "translated_sentence: \n",
            " [start] toma una [end]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "input_sentence: \n",
            " The first thing we should do is write and thank your brother-in-law for his hospitality. \n",
            "\n",
            "translated_sentence: \n",
            " [start] la primera cosa que debemos hacer es escribir y sus de por su a [UNK] [end]\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtGAxecIRWw4"
      },
      "source": [
        "English_Sentence = \"I completely forgot.\""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QvOst0wReB6"
      },
      "source": [
        "Translated_To_Spanish = decode_sequence(English_Sentence)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbCnF-UoJFq0",
        "outputId": "1256d8ba-64b2-4818-ce6a-d7da1f0a5b04"
      },
      "source": [
        "print(\"English_Sentence: \\n\", English_Sentence, \"\\n\")\n",
        "print(\"-\"*100,  \"\\n\")\n",
        "print(\"Translated_To_Spanish: \\n\", Translated_To_Spanish)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English_Sentence: \n",
            " I completely forgot. \n",
            "\n",
            "---------------------------------------------------------------------------------------------------- \n",
            "\n",
            "Translated_To_Spanish: \n",
            " [start] me ha olvidado [end]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVb64YDfSY9T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}